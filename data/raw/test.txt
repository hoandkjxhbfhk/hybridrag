 The BEIR dataset is a large, heterogeneous benchmark for Information Retrieval (IR), garnering considerable
 attention within the research community. However, BEIR and analogous datasets are predominantly restricted
 to English language. Our objective is to establish extensive large-scale resources for IR in the Polish language,
 thereby advancing the research in this NLP area. In this work, inspired by mMARCO and Mr. TyDi datasets, we
 arXiv:2305.19840v2  [cs.IR]  16 May 2024
 translated all accessible open IR datasets into Polish, and we introduced the BEIR-PL benchmark– a new benchmark
 which comprises 13 datasets, facilitating further development, training and evaluation of modern Polish language
 models for IR tasks. We executed an evaluation and comparison of numerous IR models on the newly introduced
 BEIR-PL benchmark. Furthermore, we publish pre-trained open IR models for Polish language, marking a pioneering
 development in this field. The BEIR-PL is included in MTEB Benchmark and also available with trained models at
 URL https://huggingface.co/clarin-knext.
 Keywords: Natural Language Processing, Information Retrieval, Semantic Similarity
 1. Introduction
 Modern natural language processing (NLP) appli
cations often require support from efficient informa
tion retrieval (IR) processes, e.g. in order to effi
ciently acquire and accurately pre-filter texts. An IR
 component is necessary in the case of many NLP
 tasks such as Question Answering, Entity Linking,
 or Abstractive Summarization. Recently, classic
 IR models based on lexical matching are typically
 combined with neural retrievers utilizing large pre
trained language models. The neural language
 models based on the transformer architecture facil
itate solving NLP problems in a multilingual setting
 due to their cross-lingual alignment originating from
 pre-training on parallel corpora. Such models have
 achieved promising results in a zero-shot setting, in
 which the model is trained only on the source lan
guage data only and evaluated on the target. Thus,
 there is a great need to create cross-lingual eval
uation data and benchmarks similar to the mono
lingual BEIR (Thakur et al., 2021) for many lan
guages. Although existing multilingual evaluation
 benchmarks try to include as many languages as
 possible, the Polish language has been much less
 prominent in IR studies focused on neural models
 due to the limited availability of Polish IR datasets.
 Theexisting studies on dense IR tasks suggest that
 denseretrieval models outperform lexical matchers,
 such as BM25 algorithm (Karpukhin et al., 2020).
 However, measuring the performance gap between
 lexical retrievers and dense retrieval models is very
 instructive, as the lexical matchers typically require
 much fewer computational resources. Moreover,
 lexical matchers are still a tough baseline for neural
 retrievers in specific domains.
 Our main goal was to create a large scale bench
mark dataset for IR in the Polish language, which
 is especially aimed at zero-shot approaches. Addi
tionally, we wanted to train and evaluate existing IR
 models known from literature, but have not been
 so thoroughly analysed on Polish datasets, yet, to
 determine their performance and establish a base
line for future research. Our contributions are as
 follows.
 • For the sake of comparison and compatibil
ity, we translated the original BEIR bench
mark datasets to the Polish language, a less
resourced language in IR, e.g., Polish was
 neither covered by the original multilingual
 MSMARCOdataset(Nguyenetal., 2016), nor
 by Mr. TyDi benchmark (Zhang et al., 2021),
 that had been a strongly limiting factor for de
veloping dense retrieval models for the Polish
 language.
 • Wefine-tuned five deep neural models for the
 re-ranking task using different model architec
tures and sizes, that are currently present in
 the literature.
 • We fine-tuned an unsupervised dense bi
encoder as a retriever on the Inverse Cloze
 Task (ICT) and compared its performance with
 an available multilingual sentence embedding
 model, as well as with lexical BM25 retrieval.
 • Wedemonstrated that both the BEIR-PL and
 original BEIR benchmarks are of heteroge
Collection
 There are 50 states in the USA
 France defeated
 Paris is known for ...
 Paris is in France
 Scuderia Ferrari is Italian ...
 French fries are from ...
 Monet was born in Paris
 Where is Paris?
 Query
 BM25
 Query
 Where is Paris?
 TopK passages
 There are 50 states in the USA
 France defeated ...
 Paris is known for ...
 Paris is in France
 Scuderia Ferrari is Italian ...
 French fries are from ...
 Monet was born in Paris
 Passage Score Rank
 1
 2
 3
 4
 5
 6
 7
 36.1
 32.8
 32.2
 24.6
 11.0
 9,8
 5.7 Reranker
 There are 50 states in the USA
 France defeated ...
 Paris is known for ...
 Paris is in France
 Scuderia Ferrari is Italian ...
 Monet was born in Paris
 Passage Score Rank
 1
 2
 3
 4
 5
 6
 7
 89.9
 71.2
 50.1
 30.8
 29.1
 7.0
 5.7
 French fries are from ...
 Results
 Rank Score
 1 89.9
 2 71.2
 3
 4 30.8
 5 29.1
 6 7.0
 7
 Figure1: Inretrievalwithre-rankingsetting, inthefirststage, top@kmost relevantdocumentsare
 retrievedbythefastbut lessaccuratemodel. Inourcase, itwasBM25.Afterward,thedocumentsare
 re-rankedbyamorepowerfulandmoreaccuratemodel.
 neousnature,andthat toaccuratelycompare
 modelperformance, it isnecessarytoclosely
 examine the results of individual datasets
 ratherthanrelyingsolelyonoverallaverages
 acrosstheentiredataset.
 •WetestedtrainedmodelsonPolEval 2022
 PassageRetrievalcompetitiondatasets,which
 coverthreedifferentdomainsandprovidesad
ditional testingfortrainedmodels.
 2. Relatedwork
 Webuiltuponthe ideaof theBEIRbenchmark
 dataset(Thakuretal.,2021),asit ispreciselyfo
cusedonthezero-shotevaluationofmodernIR
 systems.Neural IRsystemsaretrainedonlarge
 datasetssuchasMSMARCO(Bonifacioetal.,
 2021), or syntheticdatasetsderived fromlarge
 pre-trainedgenerativelanguagemodels(Bonifacio
 etal.,2022).MSMARCOhasbeentranslatedinto
 manydifferent languages(Bonifacioetal.,2021),
 butnot toPolish, yet. Moreover,evenotherex
tensivemultilingual benchmarks for IRsuchas
 Mr.TyDi(Zhangetal.,2021)–coveringmanytopo
logicallydiverselanguages–donot includePolish
 dataandincludessofaronlyoneSlaviclanguage,
 namelyRussian.ThereistheMTEBBenchmark
 (Muennighoffetal.,2022),whichisabenchmark
 dedicatedsemanticembeddings,whereBEIR-PL
 isincludedaspartofthePolishretrievaldatasets.
 Thetwomostcommonlyusedandrecentbench
marksforthePolishlanguagetechnology,namely
 KLEJ(Rybaketal.,2020)andLepiszcze(Augusty
niaketal.,2022)containevaluationdataformany
 distinctNLPtasks,butnoneofthemisdirectlyre
latedtoIRtasks. Therecentlypublished large
 resourceforIRinPolishlanguage,MaupQA(Ry
bak,2023),providesnearly400kquestion-passage
 pairscollected fromdifferent resources. More
over,therewasPassageRetrieval taskinPolEval
 2022competition(ŁukaszKobylińskietal.,2023),
 drivingresearchinopendomainIRforPolishlan
guagepushingresearchers todevelopeffective
 solutions(Pokrywka,2023;Kozłowski,2023;Woj
tasik,2023).
 2.1. PassageRetrieval
 The taskof textual IR is tosearch for and re
turndocuments(i.e.anyindexedtextobjects)that
 are relevant toauser query fromacollection.
 Collectionsmayconsistofmillionsofdocuments,
 whichmakesthetaskcomputationally intensive.
 Moreover,documentsandqueriesmostlyareof
 significantlydifferent lengths, thelanguageused
 throughout thedocumentsmayvary(e.g., from
 general tospecialized),andtheinformationrepre
sentedinacollectionmaycoverabroadrange
 of topics. Lexical approaches, e.g., TF.IDFor
 BM25(RobertsonandZaragoza,2009),havedom
inatedtextual IRformanyyears. Mainlydueto
 manageablecomputationalcost,butstilloffering
 decent performance. Recently, astrong trend
 hasbeenobservedtowardsdevelopingneuralre
trievermodelsthatshouldoutperformlexicalap
proaches.PretrainedlanguagemodelslikeBERT
 (Devlinetal.,2019)appearedtobeagoodbasis
 fordenseretrievalapproaches.Bi-encoderarchi
tectureaspresentedindensepassageretriever
 (DPR)(Karpukhinetal.,2020)andsentenceBERT
 (ReimersandGurevych,2019)arecommonlyused
 andexpresshighperformance,especiallyonin
domaindatasets.Thequeryanddocumentarerep
resentedbysinglevectors,whichfacilitatesapplica
tionsof fastvectordatabasesi.e.FAISS(Johnson
 etal.,2017).Themaindrawbackofsuchmodelsis
 theirlowerperformanceonout-of-domaindata.On
 theotherhand,theBM25approachachievesbetter
 resultsinsuchscenario.Apotentialapproachin
volvesutilisingamulti-vectorrepresentationofthe
 queryanddocument,asexemplifiedinColBERT
 (KhattabandZaharia,2020).Theseapproaches
 utilisethelateinteractionparadigm.ColBERTen
codesdocumentsandqueriesinmultiplevectors,
 whereeachoutputvectorcorrespondstotheinput
 token.Duringinferencetime,ColBERTcomputes
 theCartesianproductbetweenqueriesanddocu
ments,whichcanenhanceretrievaloutcomes,but
 alsonecessitatesstoringahugeindexinmemory.
 Toimproveperformanceofsinglevectorrepresen
tations, itwasshownthat languagemodelshave
 structuralreadiness,andit ispossibletopre-train
 themodel towardsbi-encoderstructure(Gaoand
 Callan,2021). Also,wecanexplicitlyaggregate
 sentence-level representationwithtokenrepresen
tation,whichisobtainedusingaweightmatrixfrom
 MaskLanguageModeling(MLM)pre-trainingtask
 andtreatingall inputsentencetokensasaproba
bilitydistributionovermodeldictionary(Linetal.,
 2022).Theaggregatedtokensrepresentation, in
 conjunctionwithCLSvector, isfine-tunedtothe
 retrieval task.
 2.2. Unsupervisedpretraining
 Unsupervisedmethodsaremainlyaimedatzero
shotschemes. InIR,mostmethodsfocusondata
 augmentationandgenerationofpseudoqueries.
 InverseClozeTask(ICT)(Leeetal.,2019)resem
blesfindingevidencetoansweraquestion. In
 contrast tothestandardClozetask–predicting
 maskedtextgivenitscontext–theICTrequires
 anticipatingthecontextgivenasentence.Theun
supervisedequivalentofthequestion-evidencepair
 isthesentence-contextpair–thecontextofasen
tencehassemanticmeaningandcanbeusedto
 infer informationthat isnotdirectlyexpressedin
 thesentence.Anothermethodofbuildingpositive
 query-documentpairinstanceswithoutsupervision
 isdropoutasaPositiveInstance(DaPI)fromSim
CSE(Gaoetal.,2021).Toperturbtheinputsen
tencerepresentation,dropout isappliedtotrans
formermodels’ fully-connectedlayersandattention
 probabilities.Asaresult,anobtainedrepresenta
tioncanbetreatedasapositiveinstancepairwith
 thesamesentencebutdifferenthiddendropout
 mask. Thepromisingperformanceofbothmeth
odsandevaluationonEnglishBEIRbenchmark
 wasshowninLaPraDor(Xuetal.,2022).
 2.3. PassageRe-ranking
 BERT(Devlinetal.,2019)enabledapproaches
 basedoncross-encoders, e.g., (Reimersand
 Gurevych,2019), inwhichweobtainajointem
beddingofadocumentandaninputquery,onthe
 token level. Inthisapproach,BERTprocesses
 adocumentandaquerysimultaneously,scoring
 theirrelationship.Duetocomputationalcost,cross
encodersareparticularlypopularintwo-stagere
trievalarchitectures. Thefirststageextractsthe
 mostrelevantdocumentswithalightandfastmodel
 (e.g., BM25(RobertsonandZaragoza,2009)).
 Cross-encodersareusedinthenextstageforre
ranking.Areranker,e.g., across-encoder,recom
putesdocumentscoresfromthefirststage(see
 Dataset #Testqueries Corpussize Avg.QLen Avg.DLen
 MSMARCO 6980 8.8M 5.33 49.63
 TREC-COVID 50 171K 9.44 137.05
 NFCorpus 323 3.6K 3.37 205.96
 NQ 3452 2.68M 7.33 66.89
 HotpotQA 7405 5.2M 15.64 38.67
 FiQA 648 57K 9.76 113.96
 ArguAna 1406 9K 168.01 142.48
 Touche-2020 49 382K 7.12 125.48
 CQADupstack 13145 547K 7.86 110.76
 Quora 10000 523K 8.13 9.85
 DBPedia 400 4.63M 4.82 41.61
 SciDocs 1000 25K 9.70 150.15
 SciFact 300 5K 11.74 187.66
 Table1: Numberofqueries,corpussize,average
 (mean)queryanddocumentwordlengthacrossall
 datasetsinBEIR-PLbenchmark.
 Figure1). Alternatively,generativesequence-to
sequencelanguagemodelswerealsoproposed
 forre-ranking.MonoT5(Nogueiraetal.,2020) is
 anadaptationof theT5model (Raffeletal.,2020)
 forIRtask.Themodelhadachievedstate-of-the
artresultsinzero-shotsetting.Thesequence-to
sequencemodelistriggeredbyapromptcontaining
 aqueryfollowedbyadocument.Themodel isex
pectedtoassesstheirrelevancebyproducing“true”
 or“false”tokeninagenerativeway.Theideaoftwo
stageapproachispresentedinFigure1. Initially,
 asetof topKdocumentsisretrievedusingtech
niquessuchasBM25.Subsequently, theretrieved
 documentsarererankedbasedonthequeryusing
 areranker.
 2.4. BEIRbenchmark
 BEIRisabenchmarkforzero-shot IRencompass
ingvarioustasks–theirsizesareshowninTable1.
 TheauthorsofBEIRbenchmarkaimedatobtaining
 alarge-scaledatacollectionrepresentingdiversi
fiedIRtasks,withvariousfeaturesof textdataand
 queries,e.g.collectingqueriesanddocumentsof
 different lengthsandstyle,alsooriginatingfrom
 differentdomains,notonlynewsorWikipediaar
ticles. Differentdomainsaremeant torepresent
 real-worlddatasettingsandshouldbechallenging
 forvariousmethods.Moreover, thedatasetswere
 annotatedbyutilisingdifferentannotationstrate
gies,e.g. performedbycrowd-workersbutalso
 expertsinspecificcases.
 3. Methodology
 Inthissection,wepresentthestepstakentocreate
 BEIR-PLbenchmarkdataset.Asouraimwasto
 buildalarge-scalebenchmarkasareferencepoint
 forcomparingdifferent IRmodels inPolish,we
 decidedtotranslatetheentireBEIRbenchmarkus
ingautomatedMachineTranslation.Subsequently,
 wetrainedandevaluatedbaselinemodelsonthe
Queries are pre-defined natural language ques
newly created resources. Baseline models will be
 publicly available to the research community. The
 selection of baseline models was dictated by re
cent advances in dense information retrieval and
 reranking models existing in the literature.
 3.1. Translation of the datasets
 DBPedia Q.
 LaBSE
 FiQa Q.
 HotpotQA Q.
 HotpotQA C.
 MSMARCOC.
 MSMARCOQ.
 SciFact Q.
 Quora Q.
 Quora C.
 0.89 0.88 0.90 0.93 0.91 0.87 0.88 0.90 0.91
 Semantic
 Strict
 0.85 0.80 0.84 0.88 0.84 0.91 0.80 0.82 0.92
 0.61 0.52 0.72 0.70 0.52 0.77 0.72 0.72 0.78
 Table 2: We have randomly chosen samples from
 datasets andassessedtheirtranslation correctness
 accuracy in two settings. Semantic setting is when
 the translated query or documentisunderstandable
 and adequate to the IR task, in a way that it does
 not change the meaning of the original text. Strict
 setting is the translation assessment done by a
 professional linguist in context of IR task evaluating
 both the semantic consistency, style and relevance
 of terminology. We also provide automated text
 similarity score measured by multilingual language
 model– LaBSE. We evaluated: queries- Q. and
 passages from corpus- C.
 To create a large-scale resource for information
 retrieval, it is necessary to obtain a significant num
ber of annotated query-passage pairs. However,
 the high cost of the annotation procedure can make
 this infeasible. Additionally, linguistic translation
 from foreign languages over millions of documents
 is both demanding and costly. As a result, machine
 translation can serve as a cost-effective solution to
 enrich resources in low-resource languages such
 as Polish. In order to process and translate the
 available BEIR benchmark’s datasets into the Pol
ish language, we used Google Translate service.
 This service has been previously used to trans
late mMarco (Bonifacio et al., 2021) dataset into
 various languages, but unfortunately, the Polish lan
guage was not included in this study. It has been
 shown during the development of mMarco, that
 the results obtained from Google Translate were
 better than the translation by the available open
source machine translation model, namely Helsinki
 (Tiedemann and Thottingal, 2020), which can be
 downloadedfromtheHuggingFace1 repository. For
 that reason, we decided to rely on Google Translate
 service.
 tions used to evaluate the performance of an IR
 system, while corpus refers to the set of documents
 that the system searches to find answers to the
 queries. Qrels, on the other hand, represent the
 relevance judgments indicating the relationships
 between the queries and documents in the corpus.
 Queries and corpus are defined and stored in a
 JSONLformatandqrelsintsvformat. Thatensures
 that our resource can be treated as an extension to
 and be fully compatible with the multilingual BEIR
 benchmark in the future.
 The size of the obtained resource, as illustrated
 in Table 1causesthatmanualverificationofit would
 be very laborious. Instead, in order to search for
 potential problems, we have selected 100 random
 queries and passages which were evaluated by
 a linguist in a Strict setting and a researcher in
 Semantic setting as shown in Table 2. Moreover,
 multilingual contextual embeddings model, namely
 LaBSe (Feng et al., 2022), has been used to com
pare source texts and their translations in automatic
 manner. We could observe high semantic similarity
 reported by LaBSe model and selective manual
 inspection proved that that most of the translations
 wereadequatetotheIRtask, butnotperfect. Errors
 wereparticularly noticed in the translation of Named
 Entities and when translated, queries sought the
 same information but had incorrect phrasing.
 3.2. Baseline models
 This section briefly describes the baseline IR mod
els we used for our evaluation study. The main
 baseline was computed using lexical matching with
 the BM25 implementation from Elasticsearch en
gine2 with Stempel Polish analysis plugin. This is
 a standard baseline method used in IR, which has
 demonstrated strong performance and computa
tional efficiency across various domains. It is also
 typically used in the first stage of retrieval, as shown
 in Figure 1. The baseline neural models can be
 divided into following categories:
 • Dense retrievers– these models are repesent
ing passages with single vectors and can sub
stitute BM25 matching. We evaluated three
 BERT-only bi-encoder models: the unsuper
vised bi-encoder based on ICT technique (Lee
 et al., 2019) with HerBERT (Mroczkowski et al.,
 2021) as its core and pre-existing multilingual
 model LaBSE (Feng et al., 2022).
 • Rerankers– these models can be used
 only as rerankers due to their computational
 inefficiency. We evaluated two HerBERT
based models and two T5-based models